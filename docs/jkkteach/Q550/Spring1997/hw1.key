
Q550 Homework 1: Linear Networks, Part 1.

*** KEY ***


1. Supervised Hebbian Learning.  Start the "pa" program as described
on pp.108-111 of PDP-III.  At this point, the network has been trained
just once on a single pattern. (Because of "clamping" the teacher
values, the outputs shown during training are not the actual output
activations.  When desired, use "tall" or "test" to see the actual
output activations.)

1A. Weight Explosion. "strain" the network several more times.  What
happens to the weights?

5 pts
***** Their magnitude grows without bound.

1B. Weight Decay.  By hand, show what would happen to weight w_11
during the first ten training trials, if weight decay were present.

[Use the equation: 
  change in w_11  =  lrate * teach_1 * act_1  -  lrate * w_11  .]  

What value does the weight approach? Prove it by setting the change in
weight to zero and solving for the weight value. How does that compare
with the weight value that one "strain" would produce if lrate = 1.0
and there was no decay?

5 pts
**** I meant w_00; forgot that array indices start at 0.  If a student
     does the correct computations but for w_11, that's okay. 
   
     input is +1, target is +1, lrate is .125

     time   w_11   change
     ----   ----   ----
       1    0.0    0.125
       2    0.125  0.1094
       3    0.2344 0.0957
       4    0.3301 0.0837
       5    0.4138 0.0732
       6    0.4871 0.0641
       7    0.5512 0.0561
       8    0.6073 0.0491
       9    0.6564 0.0429
      10    0.6993 0.0376

     (If there was no decay, then the change in weight would be 0.125
     on every time step.)

     Although it was not asked for in the homework, here's a graph of the
     weight values for the first 50 trials, for your edification.  (The 
     weight values were generated by a program I wrote, and graphed using
     gnuplot -- the PDP software cannot do this.)

     1 ++-----+------+-----+------+------+---AA-AA-AAA-AAA-AAA-AAA-AA-AAA-A+
       +      +      +     +      A AAA AAA A   +      +     +      +      +
   0.9 ++                    AAA A                                        ++
       |                  AA                                               |
   0.8 ++              A A                                                ++
       |              A                                                    |
   0.7 ++          A A                                                    ++
       |          A                                                        |
   0.6 ++        A                                                        ++
       |       A                                                           |
   0.5 ++                                                                 ++
       |      A                                                            |
       |    A                                                              |
   0.4 ++                                                                 ++
       |   A                                                               |
   0.3 ++                                                                 ++
       |  A                                                                |
   0.2 ++                                                                 ++
       |A                                                                  |
   0.1 ++                                                                 ++
       +      +      +     +      +      +      +      +     +      +      +
     0 A+-----+------+-----+------+------+------+------+-----+------+-----++
       0      5     10    15     20     25     30     35    40     45     50


     Clearly the weight value approaches 1.0.


     Proof:  Change in w = lrate * target * input - lrate * w
                         = lrate * 1      * 1     - lrate * w
                         = lrate * ( 1 - w )

     At asymptote, the change is zero, so
                       0 = lrate * ( 1 - w )
                       0 = 1 - w
                       w = 1

     That is the same value as one "strain" produces with no decay.


1C. Orthogonal or Linearly-Independent Training Patterns.  Do question
Q.4.2.1 on p.112 of PDP-III.  (You are encouraged to read and
understand the answer in the back of the book, but write your own
answer in your own words.)

5 pts
**** Orthogonal patterns are learned perfectly in the first epoch of
     training.  Additional training increases strengths so that the actual
     outputs are all off by an equivalent factor.

     The non-orthogonal linearly independent patterns are not learned
     perfectly.  The actual outputs produced are contaminated by other
     similar outputs.  Additional training increases the connection
     strengths but does not affect the degree of contamination.


1D. Generalization. "reset" the network, and "get" the pattern file
"one.pat". Then "strain" once.  Now "test" the network, using the "E"
option.  Use a variety of input patterns (and always using the target
pattern #0).  How are the output patterns related to the target
pattern? Why? That is, of what general principle is this a case?

5 pts
**** No matter what is input, the output is always a multiple of the
     single target pattern (i.e., a linear combination of the teacher
     patterns, and in this case there is only one teacher pattern).

     e.g. ++++++++ ==> 00000000 =  0*t
          -+-+-+-+ ==> --++--++ = -1*t


2. Hebbian vs. Delta-Rule Learning.  Use the "pa" program to study a
network with just two input nodes and one output node.  (It's easiest
to use the 8x8.tem file already supplied, and just ignore the extra
nodes.)  The first input node represents the presence or absence of a
tone (coded with activation values of +1 or 0), and the second input
node represents the presence or absence of a light (coded with
activation values of +1 or 0).  The output node represents the
presence of food or of shock (coded with activation values of +1 or
-1, not 0).  Use an "lrate" of 0.10.

2A. Blocking. You will train the network in successive phases, without
resetting the weights between phases. Make sure the network is in
delta-rule mode. First, train the network using an input of tone alone
[+1,0] and a teacher of food [+1], until "tss" is close to zero.
Next, without resetting the weights, train the network using an input
of tone and light [+1,+1] and a teacher of food [+1], for 10 epochs.
At this point, we might hope that the network will have learned to
associate both tone (alone) and light (alone) with food, because it
has been trained using both tone and light. "test" the network on tone
alone and on light alone.  Why has the learning of the light-food
association been impeded, or blocked?  Now reset the network and put
it in Hebb mode, and repeat the training and testing.  Is the
light-food association blocked in Hebbian learning?

5 pts
**** In Hebb mode, the weight from light grows to a large value in the
     second phase of training, but in delta-rule mode it does not.  The
     reason it does not grow in delta-rule mode is that the error in the
     second phase of training is very small; the learning from the first
     phase already makes the correct predictions.


2B. Apparent Base-Rate Neglect. Set up a pattern file with the
following 16 patterns in it:

  input           teacher  frequency of pattern pair
  ------------    -------  -------------------------
  tone (alone)    food     4
  tone (alone)    shock    2
  light (alone)   food     1
  light (alone)   shock    5
  tone+light      food     1
  tone+light      shock    3

How often does food occur?  How often does shock occur?  The relative
probabilities of food and shock are called their "base rates."  Here,
the base rate of shock is higher than the base rate of food; i.e.,
food is relatively rare.  

Show that the probability of food, given tone, is 0.50.  Show that the
probability of shock, given tone, is 0.50.  That might lead us to
expect that the weight from tone to the output should remain at zero,
since tone is paired with shock as often as it is paired with food.

2 pts
**** There are 10 patterns in which tone occurs.  Of those, 5 result
     in food, and 5 result in shock.  Hence the probability of shock, given
     tone, is 5/10, or 0.50.


Put the network in Hebb mode, reset the weights, and "ptrain" (NOT
"strain"!) for 10 epochs.  What is the weight from tone?  Why?

3 pts
**** The weight should be zero, because the number of patterns that
     cause the weight to increase are exactly counter-acted by the number
     of patterns that make the weight decrease.


Now put the network in delta-rule mode, reset the weights, and
"ptrain" the network for 20 epochs (at which point the "tss" will be
about 12 or 13).  Why is the "tss" not zero?  Why is the weight from
tone not zero?  (This is called "apparent base-rate neglect" because
it's as if the network has ignored the low base rate of food -- it
shouldn't predict a rare event.)

5 pts
**** The tss is not zero because the patterns are mutually
     inconsistent.  There is no set of weights that can predict that tone
     leads to food AND that tone leads to shock.  So the network must try
     to find the best average prediction.

5 pts
**** The weight from tone is not zero because the trials on which the
     weight increases are not exactly counter-acted by the trials on which
     the weight decreases (as was the case for Hebbian learning).  And that
     is because the errors are of different magnitudes on increase-weight
     trials and decrease-weights trials.  More after the next question.


To answer that last question more insightfully, try this simpler case.
Set up a pattern file with just two patterns in it:

  tone (alone) => food
  tone+light   => shock.

What is the probability of food given tone?  

2 pts
**** p(f|t) = 1/2, or 0.50.


Put the network in Hebb mode and train the network for 10 epochs.
What is the weight from tone?

3 pts
**** It is zero (because increase-weight trials are counter-acted by
     decrease-weight trials).


Now, put the network in delta-rule mode, reset the weights, and
"ptrain" the network until the "tss" goes to zero.  Why is the weight
from tone not equal to zero?  (HINT: Relate this to blocking!)

5 pts
**** The weight is not zero because the errors on increase-weight
     trials are not equal to the errors on decrease-weight trials.  And
     that, in turn, is because the weight from LIGHT grows to a negative
     value very quickly.  In detail: The weight from LIGHT grows to a
     negative value quickly because it is not contradicted by any other
     patterns (across all patterns, light always and only occurs with
     shock).  Consequently, on TONE+LIGHT trials, the network predicts
     shock, and the error is relatively small, and hence the weight from
     TONE decreases only a little.  Thus, the rapid learning of the
     LIGHT-->SHOCK association *blocks* learning of the TONE-->SHOCK
     association.  Then, on the TONE alone trial, the error is relatively
     large, and the weight increases a lot.

     The same situation arises in the sixteen-pattern set:  The weight
     from LIGHT grows to a negative value very quickly, and blocks learning
     of TONE-->SHOCK on TONE+LIGHT-->SHOCK trials.

=== end ===
