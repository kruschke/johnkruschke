=== KEY ===

> Q550 Homework 4: Backpropagation
> 
> For this exercise, you are to study the 4-4 encoder problem with 1, 2
> or 10 hidden nodes.  Sample files for the 10-node case are included
> below.  You should set up analogous files for the 1-node and 2-node
> cases.  Use the "4n4trn.pat" pattern file for training, and the
> "4n4gen.pat" file for generalization tests.  Note that for the "bp"
> program, teacher values of -1 mean no particular desired value; i.e.,
> no error is generated for that output node regardless of the actual
> output activation.  Make sure that the startup file is the same in
> every case, except for the command that calls the network file.
> 
> 1. Learning Speed as a Function of Number of Hidden Nodes.  
> 
> For each of the three sizes, "ptrain" the network for 500 epochs, and
> log the "tss" as a function of epoch.  (The training automatically
> stops when "ecrit" is reached, so you might need to reduce "ecrit" to
> train for 500 epochs.  Also, you can set the "stepsize" to "epoch" so
> that less time is wasted with screen updates.)
> 
> 1A. Make a graph of the three learning curves. 

5 pts.

This graph was made using 'gnuplot' on a Unix system; you can use
whatever graphing device you like.  The graphs will be a little
different for each run, depending on the random initial weight values
and the random order of training.  For example, this graph shows a
plateau in the learning curve for the 4two4 network (at a tss value
just less than 1.0) which doesn't always show up so strongly.

 4.5 ++-----+------+-----+------+------+------+------+-----+------+-----++
     A      +      +     +      +      +      +      +     +      +      +
   4 ++                                                 '4ten4.log'  A  ++
     C                                                  '4two4.log'  B   |
     |                                                  '4one4.log'  C   |
 3.5 AA                                                                 ++
     BA                                                                  |
   3 +AB                                                                ++
     |AA                                                                 |
 2.5 ++BB                                                               ++
     | ABC                                                               |
     | ABBCCCCC  CCCC       C       C                                    |
   2 ++A BCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC       ++
     | A BBB       C  C  CCC CCCCC CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC  |
 1.5 ++A  BBBB                                                       CCCCC
     |  A   BBBB                                                         |
   1 ++ A     BBBB                                                      ++
     |  A        BBBBBBBBBB                                              |
     |   A                BBB                                            |
 0.5 ++  A                  BB                                          ++
     +   A  +      +     +   BBBBB     +      +      +     +      +      +
   0 ++--AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
     0     50     100   150    200    250    300    350   400    450    500

> 1B. Do all three networks learn the mapping? That is, do all three
> networks reduce "tss" to nearly zero? Why or why not?

10 pts.

Only the two- and ten-node networks learn the mapping.  

The one-node network is stuck at a "tss" of about 1.6, because it can
only learn two of the four patterns (i.e., it gets two patterns
wrong).  The reason that the one-node network can learn only two
patterns is clear by considering the distribution of the FOUR pattern
activations in the ONE-dimensional hidden space.  The four training
inputs are represented by four values of the hidden node activation.
The output nodes, "looking down" on the one-dimensional hidden space,
can only make a linear slice through that space.  Thus, they can only
carve off the two extreme activations, and not the two intermediate
ones.

The two-node network can learn the mapping because the four training
patterns are distributed to corners of the two-dimensional hidden
space, and the output nodes looking down on that space can carve off
individual corners.

For the ten-node network, there is even more "room" in the
ten-dimensional hidden layer representation for the output nodes to
carve off individually the four training patterns.

(Note: As described in class, the N-N encoder problem can be solved
with just 2 hidden nodes, for any size N, at least in principle.)

> 1C. If both the 2-node and 10-node networks can solve the problem, why
> is the 10-node network faster?

10 pts.

More hidden nodes are faster than fewer because comparable magnitude
weight changes are happening on all the connections, so more
connections implies more error reduction.

Another factor is that the dimensionality of the weight space is
larger, so that there might be more paths/valleys to more solution
points, so that a given random starting point is more likely have a
short path to a solution.

> 2.  Generalization as a Function of Number of Hidden Nodes.
> 
> After training each size network for 500 epochs, "tall" the patterns
> (use "4n4gen.pat").
> 
> 2A. Make a table showing the four output activations for each pattern,
> for each network.  (Each table has 16 rows, one per pattern, and 5
> columns: The first column specifies the input pattern, the next four
> columns specify the output activations.)

10 pts.

Only the patterns indicated with an asterisk were trained.
 
pattern		1 hidden	2 hidden	3 hidden
-------		--------	--------	--------
0000		03 22 02 20	03 03 01 01	08 05 07 06

0001*		02 23 06 21	00 02 03 96	00 00 00 98
0010*		00 26 90 25	03 00 96 01	00 00 98 00
0100*		02 23 06 21	02 96 00 02	00 98 00 00
1000*		97 16 00 15	96 02 02 00	98 00 00 00

0011		00 26 90 25	00 01 06 95	00 00 79 68
0101		01 23 12 22	00 88 00 11 	00 63 00 86
1001		97 16 00 15	09 03 01 00	77 00 00 72
0110		00 26 90 25	00 14 00 20	00 84 63 00
1010		09 21 00 20	26 00 88 00	78 00 75 00
1100		97 16 00 15	87 11 00 00	66 78 00 00

0111		00 26 91 25	00 02 03 97	00 44 33 37
1011		04 22 01 20	04 00 97 01	24 00 23 49
1101		97 16 00 15	03 96 00 02	35 63 00 39
1110		05 22 01 20	95 02 01 00	41 42 55 00

1111		02 22 04 21	00 18 00 08	08 09 05 14

(Note: The exact activation values you get will be a little different,
depending on the random starting weights and random training
sequence.)

> 2B. Do the networks generalize the same way?  Why or why not?  Can you
> discern any regularities in the generalization?

10 pts.

The network with 1 hidden node can only learn to respond correctly to
two of the trained patterns, as described earlier.  In this case 0010
and 1000 were learned.  When one of the novel patterns is tested, the
net's response depends on which of the input positions are active.  If
both of the learned positions are active (i.e. 1 and 3) they tend to
cancel each other out, producing a set of low output activations, but
if one and not the other is active then the output exactly matches the
output to the trained, related encoder pattern.  When neither of the
learned input positions are active then the outputs are all low.

As in the case of the one-node network, the two-node network is best
understood by considering the way the output nodes carve up the hidden
node space, and how the hidden nodes distribute the input patterns.
The weights from the input nodes to the two hidden nodes are usually
something like ++-- and +-+-, so that the four training patterns are
mapped to the four corners of the hidden-node activation space.  Each
output node then carves off one corner of the space.  For novel
patterns, the hidden activations can end up being in one of the
corners or in the central "no-man's land," and so typically either
just one or none of the output nodes will be activated.
 
The network with 10 hidden nodes produced generalizations that
mirrored the input very closely--every active input node was
represented by the corresponding moderately active output node.  Note
that as the number of active inputs _increases_, the total activation
of the output _decreases_, as addressed in question 2C (below).

> 2C. When the input is 1 1 1 1, why is the activation of every output
> node close to zero?

5 pts.

For any output node, it's trained to be zero for three out of four
training patterns.  Thus, when all the inputs are 1 (a sum of all the
training inputs), the network gives the summed (sort-of) responses --
essentially zero.

=== END ===
