<HTML>
<font face="tahoma">
<HEAD>
<TITLE>P747 Intro. to Bayesian Statistics, Prof. Kruschke</TITLE>
</HEAD>
<BODY bgcolor="khaki">


<h2>P747<br>Introduction to Bayesian Statistics.</h2>

<h3>
<a href="http://www.indiana.edu/~kruschke/">
Prof. John K. Kruschke</a>
</h3>

<p><b>This describes the 2005 version of the seminar. The 2006 version
will use difference materials.</b>

<p><font size="-1">This web page is at URL = 
http://www.indiana.edu/~jkkteach/P747_2005/</font>


<p><strong>Brief Description:</strong> This course is an introduction
to Bayesian data analysis. We'll try to cover the first 11 chapters of
the textbook (see below). We will do a lot of computer simulation, not
just mathematical theory.

<br>Format: This is a one-time seminar course, which means that class
periods will be devoted to impromptu lectures, discussions of
assignments, and computer examples. There will be regular
homeworks. Peer grading of homework is an integral part of the course,
because studying the key (generated by the instructor) and grading a
peer's paper will enhance learning.



<p><strong>Pre-requisites.</strong> Students should have already taken
a thorough course in "hypothesis testing" statistics (such as Psych
P553, and preferably also P554, or equivalent). Familiarity with basic
integral calculus and linear algebra will help. We will also be
programming statistical analyses, so previous experience with a
language such as Matlab will help.

<p><strong>Time and Place:</strong> Fall semester, 2005. Wednesdays
9:00-11:00am. Psychology Building, room 111.

<p><strong>Required Textbook:</strong> 

<br>
<a href="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C388X">
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin,
D. B. (2004). <em>Bayesian Data Analysis, 2nd Edition</em>. Boca
Raton, FL: CRC Press.
</a>

<p><blockquote><small> We will not be using the books listed below,
but if you would like supplementary material, you might consider this
less mathematical and lower level introduction: <li> Berry,
D. A. (1996). <em>Statistics: A Bayesian Perspective</em>. Belmont,
CA: Duxbury Press.  <br>Or, this excellent but very elementary
introduction to statistics with a few final chapters on Bayesian
methods: <li> Albert, J. H. and Rossman, A. J. (2001). <em>Workshop
Statistics: Discovery with Data, a Bayesian Approach.</em> Emeryville,
CA: Key College Publishing.  </small></blockquote>




<p><strong>Homework:</strong> Regular homework will be assigned. Some
exercises will be from the textbook. The authors have selected answers
available at their website.

<br><li> Textbook author's web page: <a
href="http://www.stat.columbia.edu/~gelman/">http://www.stat.columbia.edu/~gelman/</a>

<p><strong>Software:</strong> We will do some introductory analyses in
Excel or Matlab, but most analyses will be done in the language "R".

<br><li> R software web page: <a
href="http://www.r-project.org/">http://www.r-project.org/</a>

<br><li> Other documents about R: <a
href="http://cran.r-project.org/other-docs.html">
http://cran.r-project.org/other-docs.html</a>



<p><strong>Oncourse:</strong> Announcements and homework keys will be
posted on <a href="http://original-oncourse.iu.edu"
target="new">Oncourse (new window).</a> Please check often.



<p><strong>Schedule.</strong> As this is a one-time (and first-time)
course, the schedule will emerge as we go. The following table is
continually under construction.

<p><center><table border="1" cellpadding="5">

<tr>
<th>Week #, Date</th>
<th>Topic, Assignment, Etc.</th>
<tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
1: Aug. 31
</td>
<td>

<li> Ch. 1: The basics. Bayes' theorem. Goals of Bayesian modeling. Algebra of probability distributions.

<li> Ch. 2: Example of Bayesian parameter estimation: Coin flip.

<li> Excel demo of coin flip example.
 
<li> 
<a href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW01.html">
Homework 1</a>. (For this and any other assignment, only enrolled
students should turn in the homework and have it graded. Others may,
of course, do the homework, but please do not turn it in.)

</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
2: Sep. 7
</td>
<td>

<li> Excel demo of coin flip example. (Probability density on a
continuum versus probability of an interval.)

<li> R version of coin flip example.

<li> Ch. 2: Doing it analytically: Conjugate priors. (Binomial=>beta;
Normal_Mean=>normal; Normal_Variance=>inverse_gamma;
Poisson=>gamma. Prior hyperparameter values from prior knowledge.)

<li> Ch. 2: Noninformative priors. (Really wide conjugate
priors. Jeffrey's parameter invariance principle. Location parameter
=> uniform prior; scale parameter => 1/&theta; prior. But is
"noninformative prior" an oxymoron?)



<li> 
<a href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW02.html">
Homework 2</a>. 


</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
3: Sep. 14
</td>
<td>

<li> Ch. 3: Models with more than one parameter. Probability
distribution over joint parameter space versus marginal probability
distributions over single parameters. Example: Estimating mean and
variance of a normal model, using (i) noninformative prior, (ii)
conjugate prior, (iii) semi-conjugte prior.


<li> Example in R of Bayesian updating of mean and standard deviation
in normal model.


<li> 
<a href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW03.html">
Homework 3</a>. 



</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
4: Sep. 21
</td>
<td>

<li> Consolidating what we've learned. Three goals of Bayesian
statistics: Estimation, Prediction, Model Comparison.

<li> We will pass over Ch. 4 for now, but we might return to it later
if needed. (Ch. 4 and Appendix B briefly consider normal
approximations to posterior distributions and convergence to normal
when N is large.)  Next week we will be pushing ahead into the
important concepts of Ch. 5.



<li> 
<a href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW04.html">
Homework 4</a>. 



</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
5: Sep. 28
</td>
<td>

<table align="right"><tr><td>
<a href="HW5.jpg"> <img src="HW5.jpg" width="250"> </a>
</tr></td></table>

<li> Ch. 5: Hierarchical models.

<li> 
<a href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW05.html">
Homework 5</a>. The figure you create in the homework might look
something like the one at the right (click it to enlarge).

</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
6: Oct. 5
</td>
<td>

<li>Continued discussion of Ch. 5 and HW 5.

</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
7: Oct. 12
</td>
<td>

<table align="right"><tr><td>
<a href="HW6.jpg"> <img src="HW6.jpg" width="250"> </a>
</tr></td></table>

<li>Ch. 6: Does the best fitting model fit the data well? Random
samples from the posterior distribution of parameter values, and
random samples from the model given those parameter values.

<li><a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW06.html">
Homework 6</a>. The figure at right shows what your output might look like.

</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
8: Oct. 19
</td>
<td>

<li>Ch. 6, continued: Does the best fitting model fit the data well? Random
samples from the posterior distribution of parameter values, and
random samples from the model given those parameter values.

<li><a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW07.html">
Homework 7</a>. 

</td>
</tr>

<!-- --------------------------------------------------- -->
<tr>
<td>
9: Oct. 26
</td>
<td>

<li>Ch. 11: Rejection sampling.

<li>Grading of HW 6 due. HW 7 due.  <a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW08.html">
Homework 8 assigned</a>. 

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
10: Nov. 2
</td>
<td>

<li>Ch. 11: Metropolis algorithm for sampling.

<li>Grading of HW 7 due. HW 8 due. <a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW09.html">
Homework 9 assigned</a>. 

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
11: Nov. 9
</td>
<td>

<li>Sampling applied to the three main goals of Bayesian modeling:
Estimation, prediction and model comparison.

<li>Grading of HW 8 due. HW 9 due. <a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW10.html">
Homework 10 assigned</a>. 

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
12: Nov. 16
</td>
<td>

<li>Ch. 12: EM algorithm for finding modes of (marginal) distributions.

<li>Ch. 18: Mixture models and application of EM algorithm.

<li>Grading of HW 9 due. HW 10 due.

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
13: Nov. 23
</td>
<td>

<li>No class this week because of Thanksgiving break. 


</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
14: Nov. 30
</td>
<td>

<li>Bayesian versus NHST approaches to data analysis: Tests of
proportions (binomial) and differences of means (t-test).

<li>Grading of HW 10 due.  <a
href="http://www.indiana.edu/~jkkteach/P747_2005/P747_Bayes_HW11.html">
Homework 11 assigned</a>. 

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
15: Dec. 7
</td>
<td>

<li>HW 11 due. Class will discuss key to that
HW. No new HW assigned.

<li>Concluding remarks and discussion.

</td>
</tr>

<!-- --------------------------------------------------- -->

<tr>
<td>
Finals Week: Wednesday Dec. 14
</td>
<td>

<li>Grading of HW 11 due. No final exam.


</td>
</tr>

<!-- --------------------------------------------------- -->


</table>
</center>

<p><hr>
</BODY>
</font></HTML>



